{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9I+oPjmUxUjLffMljC9P2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ulingga/Manchu-English_babyMT/blob/main/Manchu_English_NMT_Tokenization_%26_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TUhqxcLP_Pr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68b46d7-1638-4a99-8f9e-c4e3aa8960cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words: 1873\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Load data\n",
        "url = 'https://raw.githubusercontent.com/ulingga/dataset/main/Manchu_English.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "source_data = df['Manchu']\n",
        "target_data = df['English']\n",
        "\n",
        "combined_text = ' '.join(source_data)\n",
        "tokens = word_tokenize(combined_text)\n",
        "\n",
        "word_counts = Counter(tokens)\n",
        "unique_words = list(word_counts.keys())\n",
        "num_unique_words = len(unique_words)\n",
        "\n",
        "print(f\"Total unique words: {num_unique_words}\")\n",
        "# print(f\"Each word's count: {word_counts}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "source_data = df['Manchu']\n",
        "target_data = df['English']\n",
        "for src, tgt in zip(source_data, target_data):  # zip() is used to pair elements from two iterables\n",
        "    data.append(\n",
        "        {\n",
        "            \"translation\": {\n",
        "                \"mc\": src.strip(),  # strip() is used to remove leading/trailing whitespace\n",
        "                \"en\": tgt.strip()\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(f'total size of data is {len(data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fo4un3-QkD-",
        "outputId": "1f671cc9-e39b-4d62-df81-954b28648297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total size of data is 771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install sentencepiece\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "E1_MIfs5WYsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    MBartForConditionalGeneration, MBartTokenizer,\n",
        "     Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "   )\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "udt14dcYXC2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XFLWFqbAXUY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "import os\n",
        "\n",
        "# 'source_data' contains Manchu texts and 'target_data' contains English translations\n",
        "texts = source_data.tolist() + target_data.tolist()  # Combine lists\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train_from_iterator(\n",
        "    texts,\n",
        "    vocab_size=5000,  # You can adjust this size as needed\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\n",
        "        \"<s>\",\n",
        "        \"<pad>\",\n",
        "        \"</s>\",\n",
        "        \"<unk>\",\n",
        "        \"<mask>\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer_path = '/content/drive/MyDrive/tokenizer'\n",
        "\n",
        "if not os.path.exists(tokenizer_path):\n",
        "    os.makedirs(tokenizer_path)\n",
        "\n",
        "# Save the trained tokenizer model\n",
        "tokenizer.save_model(tokenizer_path)\n",
        "\n",
        "# Specify the file name for the JSON\n",
        "tokenizer_json_path = os.path.join(tokenizer_path, \"tokenizer.json\")\n",
        "\n",
        "# Save the tokenizer's JSON\n",
        "tokenizer.save(tokenizer_json_path, pretty=True)  # Saves tokenizer configuration in JSON format"
      ],
      "metadata": {
        "id": "D9YpkM-GXKQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Specify the folder where the tokenizer files are stored (vocab.json, merges.txt, tokenizer.json).\n",
        "tokenizer_path = \"/content/drive/MyDrive/tokenizer\"  # Adjust this path if necessary.\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
        "\n",
        "# Test the tokenizer\n",
        "encoded_input = tokenizer.encode(\"orin de jiyanggÅ«n ing iliha\", \"the General built camp on the 20th\")\n",
        "decoded_output = tokenizer.decode(encoded_input)  # 'ids' attribute is not required here.\n",
        "print(decoded_output)\n"
      ],
      "metadata": {
        "id": "pA-FGzJ3Xgzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"/content/drive/MyDrive/tokenizer/tokenizer.json\")  # Adjust the path if needed\n",
        "\n",
        "# Ensure the \"<pad>\" token is set correctly\n",
        "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"<pad>\"), pad_token=\"<pad>\")\n",
        "\n",
        "# Load into the transformers' PreTrainedTokenizerFast\n",
        "transformers_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
        "\n",
        "# Verify if the padding token is set correctly now\n",
        "print(transformers_tokenizer.pad_token_id)  # Should not be `None`\n",
        "print(transformers_tokenizer.pad_token)    # Should be \"<pad>\"\n",
        "\n",
        "# Define a function to encode the texts\n",
        "def encode_data(text):\n",
        "    return transformers_tokenizer(text, padding='max_length', truncation=True, max_length=256)\n",
        "\n",
        "# Now apply the function using a list comprehension or similar method\n",
        "source_data_tokenized = [encode_data(text) for text in source_data.tolist()]\n",
        "target_data_tokenized = [encode_data(text) for text in target_data.tolist()]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-H4QkW1cj5Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_for_model(example):\n",
        "    # Tokenize the Manchu text and the English translation\n",
        "    # This will give you the input_ids necessary for the model\n",
        "    mc_tokenized = transformers_tokenizer.encode_plus(example['translation']['mc'], padding='max_length', truncation=True, max_length=128)\n",
        "    en_tokenized = transformers_tokenizer.encode_plus(example['translation']['en'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": mc_tokenized['input_ids'],  # Manchu tokens, serving as the model input\n",
        "        \"attention_mask\": mc_tokenized['attention_mask'],  # Attention mask for the input (optional, but often helpful)\n",
        "        \"labels\": en_tokenized['input_ids']  # English tokens, serving as the expected model output (labels)\n",
        "    }\n",
        "\n",
        "# Now, tokenize the entire dataset\n",
        "tokenized_dataset = [tokenize_for_model(example) for example in data]\n"
      ],
      "metadata": {
        "id": "UKjIYa1ARJ3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "\n",
        "# Create a Dataset from your processed data\n",
        "tokenized_dataset = Dataset.from_dict({\n",
        "    'input_ids': [entry['input_ids'] for entry in tokenized_dataset],\n",
        "    'attention_mask': [entry['attention_mask'] for entry in tokenized_dataset],  # Optional\n",
        "    'labels': [entry['labels'] for entry in tokenized_dataset],\n",
        "})\n",
        "\n",
        "# Now, you can proceed with your train-test split as you did before.\n",
        "\n",
        "# Calculate the size of each dataset split, based on the proportions.\n",
        "total_size = len(tokenized_dataset)\n",
        "train_size = int(0.75 * total_size)  # 75% for training\n",
        "test_size = int(0.15 * total_size)  # 15% for testing\n",
        "val_size = total_size - train_size - test_size  # Remaining 10% for validation\n",
        "\n",
        "# Generate random but non-overlapping indices for each split.\n",
        "indices = np.random.permutation(total_size)\n",
        "train_indices = indices[:train_size]\n",
        "test_indices = indices[train_size:train_size + test_size]\n",
        "val_indices = indices[train_size + test_size:]\n",
        "\n",
        "# Create data subsets using the `.select()` method and the indices.\n",
        "train_dataset = tokenized_dataset.select(train_indices)\n",
        "test_dataset = tokenized_dataset.select(test_indices)\n",
        "val_dataset = tokenized_dataset.select(val_indices)"
      ],
      "metadata": {
        "id": "RokARbZrT1u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare your data: each 'input_ids' and 'labels' should be a flat value, not a list.\n",
        "# Here is a simplified transformation. Adjust as necessary for your specific case.\n",
        "\n",
        "def transform_data_for_dataset(data_entry):\n",
        "    input_ids_list = data_entry['input_ids']  # Ensure this is actually a list of ints\n",
        "    labels_int = data_entry['labels']  # Assuming labels are single values (e.g., for classification)\n",
        "    return {'input_ids': input_ids_list, 'labels': labels_int}\n",
        "\n",
        "transformed_data = [transform_data_for_dataset(entry) for entry in tokenized_dataset]\n",
        "\n",
        "\n",
        "# Now, you have train_dataset, test_dataset, and val_dataset for training, testing, and validation, respectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "VNzyS7JXy-Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Union\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "# device = torch.device(\"cpu\") # remove this if you have a GPU\n",
        "\n",
        "class CustomDataCollator:\n",
        "    # 'tokenizer' is the parameter name in the method definition.\n",
        "    # It doesn't have to match the variable name outside of this method.\n",
        "    def __init__(self, tokenizer, pad_token_id: int, pad_attention_mask: bool = False):\n",
        "        self.tokenizer = tokenizer  # Here we use 'tokenizer', the name of the parameter, not the external variable name.\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.pad_attention_mask = pad_attention_mask\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = []\n",
        "        attention_masks = []  # Initialize even if they might not be used\n",
        "        labels = []\n",
        "        batch_to_return = {}\n",
        "\n",
        "        for feature in features:\n",
        "            ids = feature.get('input_ids')\n",
        "            # Standardizing input_ids to be a Tensor, if it's not already\n",
        "            if isinstance(ids, list):\n",
        "                ids = torch.tensor(ids, dtype=torch.long)\n",
        "            elif not isinstance(ids, torch.Tensor):\n",
        "                raise ValueError(f\"Unexpected type for input_ids: {type(ids)}. Each 'input_ids' should be a list or a Tensor.\")\n",
        "\n",
        "            input_ids.append(ids)\n",
        "\n",
        "            # If attention_mask is to be processed, either use existing or compute it\n",
        "            if self.pad_attention_mask:\n",
        "                attention_mask = feature.get('attention_mask')\n",
        "                if attention_mask is None:  # Compute attention_mask if it doesn't exist\n",
        "                    attention_mask = [1] * len(ids)\n",
        "                elif isinstance(attention_mask, list):\n",
        "                    attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "\n",
        "                attention_masks.append(attention_mask)\n",
        "\n",
        "        input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n",
        "        labels = [torch.tensor(feature['labels'], dtype=torch.long) for feature in features]\n",
        "        labels_padded = pad_sequence(labels, batch_first=True, padding_value=self.pad_token_id)\n",
        "\n",
        "        batch_to_return['labels'] = labels_padded\n",
        "        batch_to_return['input_ids'] = input_ids_padded\n",
        "\n",
        "        if self.pad_attention_mask:\n",
        "            attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)  # 0 is standard for attention masks\n",
        "            batch_to_return['attention_mask'] = attention_masks_padded\n",
        "\n",
        "        return batch_to_return\n"
      ],
      "metadata": {
        "id": "I9csur4Z297K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"/content/drive/MyDrive/my_tokenizer\"\n",
        "\n",
        "# Save your tokenizer to the directory.\n",
        "transformers_tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "# Load model\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
        "\n",
        "# Ensure the tokenizer uses the correct pad token\n",
        "tokenizer.pad_token = transformers_tokenizer.eos_token  # mBART uses the <eos> token as the padding token\n",
        "pad_token_id = transformers_tokenizer.pad_token_id  # gets the pad token id which is recognized by the tokenizer\n",
        "\n",
        "\n",
        "# 'transformers_tokenizer' is the variable name in the outer scope.\n",
        "data_collator = CustomDataCollator(tokenizer=transformers_tokenizer, pad_token_id=pad_token_id, pad_attention_mask=True) # or False, based on your preference\n",
        "\n",
        "# torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "fEEyvKkPyzDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments setting\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/fine_tune_mbart\",  # The directory where the model predictions and checkpoints will be written.\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=1,  # Adjust based on your GPU's capabilities\n",
        "    per_device_eval_batch_size=1,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=100,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    warmup_steps=100,\n",
        "    num_train_epochs=1,  # You can adjust the number of training epochs\n",
        "    fp16=True,  # If you have a supported GPU, you can enable fp16 for faster training\n",
        ")\n",
        "\n",
        "# Check if GPU is available and use it\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZspXUdO_LVrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=transformers_tokenizer,\n",
        "\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Define model's name\n",
        "model_name = \"manchu_to_english_babyMT\"\n",
        "\n",
        "# Save the model\n",
        "model_save_path = f\"/content/drive/MyDrive/fine_tune_mbart/{model_name}\"\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Specify the path to your model\n",
        "path_to_model = \"/content/drive/MyDrive/fine_tune_mbart/finished_model\"\n",
        "\n",
        "# Load the model and tokenizer from the saved model directory\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "\n",
        "# Now you can use 'model' and 'tokenizer' with your data for inference.\n",
        "model.save_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "GVVBPYKuKIgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "# Ensure this path points to the directory where the model and tokenizer were saved\n",
        "path_to_saved_model = \"/content/drive/MyDrive/fine_tune_mbart/finished_model\"  # Update with your path\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(path_to_saved_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_saved_model)\n",
        "\n",
        "# Sample sentence for translation\n",
        "sample_sentence = \"dergici. hafan coohai sain be fonjiha.\"\n",
        "\n",
        "# Encode the sentence\n",
        "inputs = tokenizer(\n",
        "    sample_sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512  # or another appropriate value based on your model's configuration\n",
        ")\n",
        "\n",
        "# Translate the sentence\n",
        "model.eval()  # Make sure the model is in evaluation mode\n",
        "with torch.no_grad():  # No need to track gradients for inference\n",
        "    outputs = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
        "\n",
        "# Decode the model output to get the translation\n",
        "translated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Translated Sentence: \", translated_sentence)\n"
      ],
      "metadata": {
        "id": "X-hz12vpArUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}